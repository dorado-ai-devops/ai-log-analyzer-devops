# ğŸ§  ai-log-analyzer-devops

> Analizador de logs con inteligencia artificial para entornos DevOps (como Jenkins o Kubernetes), que proporciona diagnÃ³sticos automÃ¡ticos y recomendaciones utilizando LLMs como GPT-4o (OpenAI) o modelos locales mediante Ollama (LLaMA3, Phi-3, etc).

---

## ğŸš€ CaracterÃ­sticas

- ğŸ” Analiza logs de pipelines CI/CD (Jenkins, Helm, Kubernetes)
- ğŸ¤– Compatible con OpenAI GPT-4o o modelos locales vÃ­a Ollama
- ğŸ›  CLI simple y microservicio REST (Flask)
- ğŸ³ Preparado para Docker y desplegable en Kubernetes
- âœï¸ Prompts modulares y personalizables
- ğŸ“ Funciona completamente online u offline

---

## ğŸ“¦ Estructura del proyecto

```
ai-log-analyzer-devops/
â”œâ”€â”€ app.py                 # Servidor Flask para integraciÃ³n REST (K8s)
â”œâ”€â”€ cli/                   # Scripts de lÃ­nea de comandos
â”œâ”€â”€ lib/                   # LÃ³gica central y clientes de IA
â”œâ”€â”€ logs/                  # Logs de ejemplo para pruebas
â”œâ”€â”€ prompts/               # Plantillas de prompts para LLMs
â”œâ”€â”€ requirements.txt       # Dependencias Python
â”œâ”€â”€ Dockerfile             # ConfiguraciÃ³n de contenedor
â””â”€â”€ README.md              # DocumentaciÃ³n principal
```

---

## ğŸ§© DescripciÃ³n detallada de los componentes

### `app.py`

Microservicio Flask con endpoint `/analyze`:
- Recibe un log vÃ­a JSON (`{ "log": "..." }`)
- Llama internamente al CLI `cli/generate.py`
- Devuelve una respuesta estructurada generada por IA
- Listo para ser desplegado en Kubernetes

### `cli/`

Script principal: `generate.py`:
- Lee un archivo `.log`
- Selecciona el modo (`openai` u `ollama`)
- Carga el prompt correspondiente
- Muestra el resultado en consola o como JSON

### `lib/`

LÃ³gica de negocio:
- Clientes para OpenAI y Ollama
- Procesamiento de logs y prompts
- SeparaciÃ³n clara entre lÃ³gica y presentaciÃ³n

### `logs/`

Logs de prueba (`example_jenkins.log`, etc.)

### `prompts/`

Plantillas de instrucciones que controlan el comportamiento de los modelos

### `requirements.txt`

InstalaciÃ³n de dependencias:

```bash
pip install -r requirements.txt
```

Incluye: `openai`, `flask`, `requests`, etc.

### `Dockerfile`

Empaqueta la aplicaciÃ³n (servidor Flask). Ãštil para entornos CI/CD, desarrollo local y Kubernetes.

---

## ğŸ› ï¸ Primeros pasos

### ğŸ” Clonar y preparar entorno

```bash
git clone https://github.com/dorado-ai-devops/ai-log-analyzer-devops.git
cd ai-log-analyzer-devops
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

### âš™ï¸ Ejecutar como CLI (modo OpenAI)

```bash
OPENAI_API_KEY=sk-xxx python3 cli/generate.py --mode openai --logfile logs/example_jenkins.log
```

### âš™ï¸ Ejecutar como microservicio REST

```bash
python3 app.py
```

### ğŸ” Ejecutar en Docker (modo Flask API)

```bash
docker build -t log-analyzer:dev .
docker run -p 5000:5000 log-analyzer:dev
```

---

## ğŸ’¡ Ejemplo de salida

```text
Fallo detectado al conectar con el servicio de base de datos.
Causa: el servicio `db.example.local` no estaba disponible al arrancar.
RecomendaciÃ³n: verifica la variable DB_HOST, las polÃ­ticas de red y el estado del servicio. Considera implementar reintentos con backoff.
```

---

## ğŸ”® PrÃ³ximos pasos

- AÃ±adir integraciÃ³n con Jenkins
- Crear manifiestos de despliegue en Kubernetes
- Soporte para respuestas streaming desde LLMs

---

## ğŸ“¸ Capturas *(prÃ³ximamente)*

IncluirÃ¡ ejemplos visuales de logs, salidas en consola y diagramas.

---

## ğŸ‘¨â€ğŸ’» Autor

- **Dani** â€“ [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ§  Inspirado por

- [LangChain](https://github.com/langchain-ai/langchain)
- [Ollama](https://ollama.com)
- [OpenAI API](https://platform.openai.com/docs)

---

## ğŸ›¡ Licencia

GNU General Public License v3.0
