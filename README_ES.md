# ğŸ§  ai-log-analyzer-devops

> Un analizador de logs potenciado por IA que procesa registros de entornos DevOps (como Jenkins o Kubernetes) y ofrece diagnÃ³sticos inteligentes y recomendaciones usando LLMs como GPT-4o (OpenAI) o modelos locales mediante Ollama (LLaMA3, Phi-3, etc).

&#x20;&#x20;

---

## ğŸš€ CaracterÃ­sticas

- ğŸ” Analiza logs de pipelines CI/CD (Jenkins, Helm, Kubernetes)
- ğŸ¤– Potenciado por OpenAI GPT-4o o modelos locales mediante Ollama
- ğŸ“¦ Herramienta CLI sencilla, lista para Docker
- âœï¸ Prompts modulares y editables
- ğŸ“ Funciona completamente online o en modo offline

---

## ğŸ“¦ Estructura del Proyecto

```
ai-log-analyzer-devops/
â”œâ”€â”€ cli/                    # Interfaz de lÃ­nea de comandos
â”œâ”€â”€ lib/                    # LÃ³gica principal y clientes de IA
â”œâ”€â”€ logs/                   # Logs de ejemplo para pruebas
â”œâ”€â”€ prompts/                # Plantillas de prompt para LLMs
â”œâ”€â”€ requirements.txt        # Dependencias de Python
â”œâ”€â”€ Dockerfile              # Docker para despliegue containerizado
â””â”€â”€ README.md               # DocumentaciÃ³n del proyecto
```

---

## ğŸ§© DescripciÃ³n detallada de los componentes

### `cli/`

Contiene los scripts ejecutables desde terminal. Por ahora, `generate.py` es el script principal que:

- Lee un archivo de log
- Llama a un LLM segÃºn el modo (`openai` u `ollama`)
- Usa la plantilla de prompt adecuada desde `prompts/`
- Muestra el resultado por pantalla

### `lib/`

Contiene el cÃ³digo central reutilizable:

- Clientes de OpenAI y Ollama
- Funciones para cargar logs, estructurar prompts y procesar la respuesta del modelo
- Esta capa separa la interfaz CLI de la lÃ³gica interna, facilitando testeo y reusabilidad

### `logs/`

Contiene archivos `.log` de ejemplo, como `example_jenkins.log`, utilizados para pruebas y desarrollo.

### `prompts/`

Define las instrucciones o "templates" que se le pasarÃ¡n al LLM. Modularizar los prompts permite:

- Cambiar el estilo de salida
- Ajustar el comportamiento del modelo
- Separar la logica de negocio del contenido IA

### `requirements.txt`

Lista de dependencias necesarias. Incluye bibliotecas como `openai`, `requests`, etc. Se instala con:

```bash
pip install -r requirements.txt
```

### `Dockerfile`

Permite contenerizar toda la aplicaciÃ³n. Ideal para despliegue en entornos CI/CD o en servidores offline con Ollama.

---

## ğŸ› ï¸ Primeros pasos

### ğŸ” ClonaciÃ³n y configuraciÃ³n

```bash
git clone https://github.com/dorado-ai-devops/ai-log-analyzer-devops.git
cd ai-log-analyzer-devops
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

---

### âš™ï¸ EjecuciÃ³n (modo OpenAI)

```bash
OPENAI_API_KEY=sk-xxx \
python3 cli/generate.py --mode openai --logfile logs/example_jenkins.log
```

### âš™ï¸ EjecuciÃ³n (modo local con Ollama)

```bash
ollama run llama3
python3 cli/generate.py --mode ollama --logfile logs/example_jenkins.log
```

---

## ğŸ’¡ Ejemplo de salida

```text
Fallo detectado al conectar con el servicio de base de datos.
Causa: servicio `db.example.local` inalcanzable durante el arranque.
RecomendaciÃ³n: revisa la variable de entorno DB_HOST, las network policies y el estado del servicio. Considera aplicar reintentos con backoff.
```

---

## ğŸ”® Siguientes pasos

-

---

## ğŸ“¸ Capturas *(prÃ³ximamente)*

Se incluirÃ¡n ejemplos visuales de logs, salida de terminal y esquemas.

---

## ğŸ‘¨â€ğŸ’» Autor

- **Dani** â€“ [@dorado-ai-devops](https://github.com/dorado-ai-devops)

---

## ğŸ§  Inspirado por

- [LangChain](https://github.com/langchain-ai/langchain)
- [Ollama](https://ollama.com)
- [OpenAI API](https://platform.openai.com/docs)

---

## ğŸ›¡ Licencia

GNU General Public License v3.0

